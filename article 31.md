# 融合知识图谱与大语言模型的科技文献复杂知识对象抽取研究 (2025年7月)

### 1. 研究目标 · 内容 · 问题 · 出发点

* **研究领域与背景、具体对象 / 数据集**
    * **研究领域与背景**：本研究处于智能情报分析领域，专注于科技文献的知识抽取。背景是，科技文献中蕴含着由实体、关系、数据等多种知识单元组成的“复杂知识对象”，它们是科技创新的重要要素。传统的知识抽取方法效率低、主观性强，而大语言模型（LLM）在专业领域存在知识盲区和性能损失。因此，研究旨在融合知识图谱（KG）的结构化领域知识与LLM的自然语言处理能力。
    * **具体对象 / 数据集**：研究以**有机太阳能电池（Organic Solar Cells, OSC）**领域为实例。数据集来源于Web of Science论文数据库和IncoPat专利数据库，共搜集3369篇论文和421篇专利，并从中筛选出高质量的核心数据集用于知识图谱构建与模型训练。

* **论文想解决的核心问题**
    * 如何高效、准确地从专业领域的科技文献中，自动抽取出由多个知识单元关联构成的**复杂知识对象**（如实验方案，包含实验原理、材料、步骤、结果等），以克服传统方法效率低和通用大模型在专业领域能力不足的问题。

* **研究动机 / 假设**
    * **研究动机**：现有知识抽取方法多集中于实体、关系等简单的扁平化知识，对实验方案这类具有层次、时序、多维关系的复杂知识对象抽取研究不足。同时，直接使用如ChatGPT等在线LLM服务存在数据隐私泄露风险，而本地化部署的开源模型需要适配特定领域才能发挥最佳效果。
    * **研究假设**：将领域知识图谱中蕴含的结构化、形式化知识作为先验知识，注入到本地化部署的大语言模型中，可以显著增强模型对科技文献复杂知识对象的理解和抽取能力，提升抽取结果的准确性和稳定性。

* **工作内容概览**
    * **领域知识图谱构建**：首先，通过轻量级本体建模方法构建了OSC领域的知识图谱模式层。然后，使用BRAT工具对核心数据集进行人工标注，并将标注结果存储在Neo4j图数据库中，完成实例层构建。
    * **大语言模型微调**：在本地部署开源大语言模型ChatGLM2-6B，利用前一步构建的知识图谱三元组生成指令数据集，并采用低秩适应（LoRA）技术对模型进行高效微调，使其适应OSC领域的知识抽取任务。
    * **复杂知识对象抽取**：设计了一种基于思维记忆（Memory of Thoughts, MOT）机制的提示构建策略。该策略从知识图谱中检索与当前任务最相关的问答（QA）对作为示例，注入到提示中。通过与微调后的模型进行多轮问答，分层、逐步地识别实体类型并抽取出具体的知识实体，最终依据本体模型将抽取的实体组合成复杂的知识对象。

### 2. 研究方法（含模型 / 技术详解）

* **理论框架与算法**
    * 本文提出一个融合知识图谱与大语言模型的三阶段框架。
    * **阶段一：领域知识图谱构建**。建立一个包含模式层（Ontology）和实例层（Triples）的OSC领域知识图谱。
    * **阶段二：模型微调**。使用实例层数据对本地化的ChatGLM2-6B模型进行LoRA微调。
    * **阶段三：复杂知识对象抽取**。通过一种结合多轮问答和MOT机制的提示工程方法，与微调后的LLM交互，实现知识抽取。
    * **抽取算法**：如论文表1所示，算法首先从知识图谱构建一个外部记忆模块（Memory）。对文献中的每个句子，通过与记忆模块进行相似度计算，检索出Top-k个QA对作为示例。然后构建提示，与LLM进行多轮问答：前几轮迭代地确定句子的实体类型（文本分类），最后一轮根据确定的类型抽取出细粒度实体。最终，所有抽取的实体根据本体组合成复杂知识对象。

* **关键模型/技术逐一说明**
    * **知识图谱构建（KG）**
        * **架构**：模式层（Ontology）定义了实体类型（语句级、词汇级、科学数据级）、关系和属性；实例层以三元组形式存储具体知识。
        * **流程**：使用BRAT工具对科技文献进行标注，经过专家审核后，将结果转换为三元组并导入Neo4j图数据库。
        * **优势**：为LLM提供了高质量、结构化的领域先验知识。
    * **大语言模型微调（ChatGLM2-6B + LoRA）**
        * **架构**：选择ChatGLM2-6B作为基础模型。使用LoRA（Low-Rank Adaptation）技术进行微调，该技术通过在模型中引入可训练的低秩矩阵（$A$和$B$）来模拟原始参数的更新量（$\Delta W=BA$），而无需改动原始的庞大参数。
        * **训练流程**：将KG中的三元组（实体, 属性, 属性值）转换为指令式QA对。在训练时，冻结ChatGLM2-6B的全部参数，仅更新降维矩阵A和升维矩阵B。
        * **优势与局限**：相比于全量微调，LoRA极大降低了计算资源消耗和训练参数量。相比P-Tuning v2，能更好避免“灾难性遗忘”。
    * **基于MOT机制的提示构建**
        * **架构**：MOT（Memory of Thoughts）机制的核心是构建一个外部记忆模块（Memory），并在生成提示时从中“回忆”相关知识。
        * **推理流程**：
            1.  **记忆构建与聚类**：将KG中的QA对作为记忆单元构建Memory。使用LDA主题模型将Memory划分为N个主题簇，以增强检索的多样性。
            2.  **样例检索**：对于一个待处理的句子（目标问题q），首先从N个记忆簇中各找出一个与q语义最相似的记忆。然后，从这N个候选中，再选出与q相似度最高的Top-k个作为最终的问答样例。相似度通过Doc2vec模型计算文本向量，再用余弦相似度进行评估。
            3.  **提示生成**：将检索到的Top-k问答样例与目标问题q组合成一个结构化的提示（Prompt），输入给LLM。格式为：“问答样例: [样例1, ..., 样例k] 目标问题: [q]”。
        * **优势**：通过上下文学习（In-Context Learning）的方式，利用高质量、高相关的样例引导LLM生成更准确、稳定的结果，有效缓解模型的“幻觉”问题。

* **重要公式**
    * **复杂知识对象抽取概率**：
        $$P(e|s,o) = P(type_1|p_1(s,o)) \cdot \dots \cdot P(type_n|p_n(s,o,type_{n-1})) \cdot P(e|p_{n+1}(s,o,type_n))$$
        该公式表示，抽取实体$e$的概率是多轮问答中，逐步正确识别实体类型（$type_1$到$type_n$）并最终抽取实体$e$的概率乘积。
    * **语义相似度计算**：
        $$sim(q, m) = cos(doc2vec(q), doc2vec(m))$$
        使用Doc2vec模型将目标问题$q$和记忆$m$转换为向量，并通过余弦相似度计算其语义相似性。
    * **评价指标**：
        $$P = \frac{M}{M+N}, \quad R = \frac{M}{M+T}, \quad F1 = \frac{2PR}{P+R}$$
        其中，$P$为准确率，$R$为召回率，$M$为正确预测的正例数，$N$为错报数，$T$为漏报数。

### 3. 实验设计与结果（含创新点验证）

* **实验 / 仿真 / 原型流程**
    1.  **数据集准备与KG构建**：收集OSC领域文献，由领域专家筛选出核心数据集。基于此数据集，通过两阶段轻量级本体建模方法构建OSC本体模型，再利用BRAT工具人工标注，最终生成包含4700个知识实体和15377个三元组的知识图谱，并存入Neo4j。
    2.  **模型部署与微调**：在NVIDIA A40 GPU环境下部署ChatGLM2-6B模型。将知识图谱三元组按5:1划分为训练集和测试集，并转换为QA对。使用训练集和LoRA技术对模型进行微调，监控损失函数直至收敛。
    3.  **MOT机制设置**：使用训练集的QA对构建外部记忆模块。通过计算困惑度确定LDA主题模型的最优主题数为30，对记忆进行聚类。训练Doc2vec模型用于计算文本向量。
    4.  **模型评估**：设计了四种模式进行对比实验：
        * **ChatGLM2-6B**: 基础模型，不进行任何微调。
        * **ChatGLM2-6B+LoRA1**: 使用由ChatGLM自身根据文本生成的指令集进行微调。
        * **ChatGLM2-6B+LoRA2**: 使用由知识图谱生成的指令集进行微调。
        * **ChatGLM2-6B+LoRA2+MOT**: 本文提出的完整方法，即在LoRA2的基础上，使用MOT机制构建提示。
    5.  **结果分析**：在测试集上运行四种模式，使用准确率（P）、召回率（R）和F1值作为评价指标，对比分析各模式的性能。同时，分析了最终模型对不同类型实体（语句级、词汇级、科学数据级）的抽取效果。

* **数据集、参数、评价指标**
    * **数据集**: OSC核心数据集（具体构成见论文表2），包括材料研究、制备方法、机理研究、结构研究四个方面的论文和专利。
    * **参数**:
        * **LoRA**: `lora_rank`=8, `lora_dropout`=0.1, `batch_size`=4, `learning_rate`=1e-4。
        * **MOT**: `k` (问答样例数)=3, LDA主题数=30。
        * **Doc2vec**: 向量维度=20, 迭代次数=10, 学习率=0.025。
    * **评价指标**: 准确率（Precision, P）、召回率（Recall, R）、F1值（F1-score）。

* **创新点如何得到验证，结果对比与可视化描述**
    * **创新点验证**：
        1.  **KG数据微调的有效性**：通过对比`ChatGLM2-6B+LoRA2`（F1=54.2%）和`ChatGLM2-6B+LoRA1`（F1=49.1%）的结果，证明了使用来自知识图谱的高质量、结构化数据进行微调，显著优于使用LLM自动生成的通用指令集。
        2.  **MOT机制的有效性**：通过对比`ChatGLM2-6B+LoRA2+MOT`（F1=60.1%）和`ChatGLM2-6B+LoRA2`（F1=54.2%）的结果，证明了在提示中注入领域知识样例能够进一步大幅提升模型的抽取性能。
    * **结果对比与可视化**：
        * **总体性能（论文表7）**：本文提出的`ChatGLM2-6B+LoRA2+MOT`方法在所有指标上均达到最优，F1值达到60.1%，相比基线模型ChatGLM2-6B（F1=47.8%）提升了12.3%。
        * **按实体类型分析（论文表8）**：语句级实体的抽取效果最好（F1=0.66），而词汇级和科学数据级实体的效果稍差。
        * **可视化**：论文图3展示了微调过程中损失函数的变化，验证了模型的收敛性。图4通过困惑度变化曲线确定了LDA模型的最佳主题数。图5使用t-SNE将记忆簇在二维空间中可视化，展示了明显的聚类结构。

* **主要实验结论与作者解释**
    * **结论**：融合知识图谱与大语言模型的抽取方法，在准确率、召回率和F1值上均优于仅依赖大语言模型的方法。知识图谱通过高质量的微调数据和提示阶段的知识注入，能有效增强LLM在专业领域的复杂知识抽取能力。
    * **作者解释**：词汇级和科学数据级实体抽取精度较低的原因在于**错误传播**。这两类实体位于本体结构的底层，需要经过更多轮次的问答才能定位，任何上一轮的分类错误都会被传播到下一轮，从而影响最终的抽取准确性。

### 4. 研究结论

* **重要发现（定量 / 定性）**
    * **定量**：本文提出的融合方法相较于单独使用ChatGLM2-6B模型，在准确率、召回率和F1值上分别提升了14.1%、10.3%和12.3%。
    * **定性**：实验证明，将知识图谱蕴含的领域知识以两种方式（LoRA微调和MOT提示注入）赋能大语言模型是一种行之有效的策略。它能显著提升模型在特定、专业领域进行细粒度知识挖掘的效率与准确性。

* **对学术或应用的意义**
    * **学术意义**：为解决专业领域复杂知识对象抽取问题提供了一个新的、有效的技术范式，探索了知识图谱与大语言模型深度融合的协同机制。
    * **应用意义**：该方法能够高效精准地从海量科技文献中挖掘深层知识，支撑数智驱动的科学发现，可应用于构建特定学科的知识库，辅助科研人员进行文献分析和情报挖掘。此外，该框架具有迭代优化的潜力，抽取出的新知识经审核后可反哺知识图谱。

### 5. 创新点列表

1.  **面向复杂知识对象的混合抽取框架**：提出了一种专为抽取科技文献中“复杂知识对象”（而非简单实体、关系）而设计的融合方法，该方法将知识图谱构建、大模型微调和基于多轮问答的抽取流程有机地结合在一起。
2.  **知识图谱对大模型的双重赋能机制**：创造性地利用知识图谱实现了对大语言模型的双重增强：
    * **微调阶段**：利用知识图谱的三元组生成高质量的指令数据集，通过LoRA对模型进行领域适配。
    * **推理阶段**：基于思维记忆（MOT）机制，在提示中动态注入从知识图谱中检索到的高置信度问答样例，实现有效的上下文学习。
3.  **支持迭代与小样本应用的潜力**：该框架支持通过循环迭代的方式提升知识抽取的整体效果（抽取结果可用于完善知识图谱）。同时，在标注数据不足时，可不进行微调，直接基于领域本体和大语言模型实现小样本乃至零样本的知识抽取，具有较好的灵活性和可移植性。