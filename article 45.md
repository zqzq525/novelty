 # SC4ANM: Identifying optimal section combinations for automated novelty prediction in academic papers (2025)

### 1. 研究目标 · 内容 · 问题 · 出发点

-   **研究领域与背景、具体对象 / 数据集**
    -   **研究领域与背景**：本文聚焦于学术论文新颖性的自动化评估。现有方法多依赖于关键词、知识实体组合或引文分析，这些方法未能充分利用论文的核心文本内容，且忽略了不同章节（如引言、方法、结果）对新颖性判断的差异化贡献。学术论文通常遵循引言、方法、结果和讨论（IMRAD）的结构，这为基于章节内容的分析提供了基础。
    -   **具体对象 / 数据集**：研究对象为学术论文的全文内容。数据集来源于 OpenReview 平台上的 ICLR 2022 和 ICLR 2023 会议的论文及其同行评审报告。初始共收集 8183 篇论文，经过数据清洗（筛选评审专家新颖性打分一致的论文）和内容匹配，最终使用了 3500 篇具有完整内容和可靠新颖性分数的论文作为研究样本。其中，专家评审报告中提供的“技术新颖性与重要性 (Technical Novelty and Significance, TNS)”分数被作为模型预测的基准标签（ground truth）。

-   **论文想解决的核心问题**
    -   核心问题是：在对学术论文进行自动化新颖性评估时，哪一个或哪些章节的组合是最高效、最准确的？论文旨在通过实验确定预测新颖性分数的最佳章节组合，以替代当前依赖于部分信息（如摘要、标题）或将全文视为同质化文本的做法。

-   **研究动机 / 假设**
    -   **研究动机**：人类专家在评审论文新颖性时，会阅读并综合不同章节的信息，而不同章节承载的新颖性信息权重不同。现有的自动化方法忽略了这一点。因此，模拟专家的阅读过程，通过分析不同章节组合来预测新颖性，有望提升自动化评估的准确性和效率。
    -   **研究假设**：论文假设存在一个最优的章节组合，使用该组合作为语言模型的输入，其预测的新颖性分数能比使用全文或其他组合更接近人类专家的判断。同时，假设引言（Introduction）、结果（Results）和讨论（Discussion）等章节比方法（Methods）等章节包含更多直接关联新颖性判断的信号。

-   **工作内容概览（精炼概述各章节核心）**
    1.  **数据收集与预处理**：从 OpenReview 平台抓取 ICLR 会议论文及同行评审报告，处理并统一评审专家给出的新颖性分数，将其转换为三分类标签（基础新颖、中等新颖、高度新颖）。
    2.  **章节结构识别**：提出一个结合精调 SciBERT 模型和 Llama3 大语言模型的混合方法，对论文全文进行解析，准确地将其内容划分到引言、方法、结果、讨论（IMRAD）四个部分。
    3.  **模型实验与分析**：
        -   **基于预训练语言模型 (PLM) 的实验**：使用五种面向长文本的 PLM（如 Longformer, SciBERT 等），将 16 种不同的章节组合（如“引言”、“引言+方法”等）作为输入，精调模型以预测新颖性分数，系统比较各组合的性能。
        -   **基于大语言模型 (LLM) 的实验**：在零样本（zero-shot）场景下，使用 GPT-3.5 和 GPT-4o，通过提示工程（prompting）的方式，测试其在不同章节组合输入下预测新颖性分数的能力。
    4.  **结果对比与结论**：对比分析 PLM 和 LLM 在所有章节组合下的表现，并与传统的基于引文的评估方法进行比较，最终确定最佳章节组合，并探讨其对自动化评审系统和研究人员的启示。

### 2. 研究方法（含模型 / 技术详解）

-   **理论框架与算法**
    -   本文将该研究问题定义为一个新的任务：**为自动化新颖性评估寻找最优章节组合 (SC4ANM)**。该任务本质上是一个文本分类问题，目标是构建一个分类模型 $f$，该模型接收一篇论文中特定章节的组合 $C$ 作为输入，输出一个预定义的新颖性分数标签 $l \in \{0, 1, 2\}$。

-   **关键模型/技术逐一说明：架构、输入输出、训练或推理流程、优势与局限**
    1.  **章节结构识别 (Section Structure Identification)**
        -   **架构**：采用一个“PLM + LLM + 人工校验”的三阶段混合流程。
        -   **流程**：
            1.  **初步分类 (PLM)**：使用在 ArXiv 和 PubMed 数据集（含30万条带标签的章节文本）上精调过的 SciBERT 模型，对论文的每个段落进行 IMRAD 四分类预测。
            2.  **置信度判断与二次识别 (LLM)**：设定一个高置信度阈值（如 0.8）。若 SciBERT 的预测概率高于此阈值，则直接采纳结果。若低于此阈值，则将该段落文本输入到 Llama3 模型中进行二次识别。
            3.  **一致性检查与人工校正**：如果 Llama3 的识别结果与 SciBERT 一致，则采纳该结果。如果两者不一致，则将该段落标记出来，进行人工最终裁定。
        -   **优势**：该方法结合了领域专用模型 (SciBERT) 的精准性和大模型 (Llama3) 的常识推理能力，并通过人工介入处理疑难案例，最大限度地保证了章节结构识别的准确性，为后续实验提供了高质量的输入数据。

    2.  **基于 PLM 的新颖性分数预测**
        -   **模型**：采用了五种为处理长文本而设计的预训练语言模型 (PLM)，包括 Longformer、BigBird、LongT5、LED 和长文本版 SciBERT。
        -   **架构**：输入不同章节组合的拼接文本，通过相应的 PLM 模型进行编码，生成代表整个输入文本的嵌入向量 (Text embedding)。该向量随后被送入一个简单的多层感知机 (MLP) 分类头，最终输出三分类（0: 基础新颖, 1: 中等新颖, 2: 高度新颖）的预测结果。
        -   **输入输出**：输入是 16 种章节组合中的一种（如仅“引言”的文本，或“引言”+“结果”+“讨论”的拼接文本）。输出是预测的新颖性类别（0, 1, 或 2）。
        -   **训练流程**：在 2500 篇论文构成的训练集上进行模型精调，使用包含 350 篇论文的验证集进行早停（early stopping）策略以防止过拟合，最后在独立的 350 篇论文测试集上评估最终性能。

    3.  **基于 LLM 的新颖性分数预测**
        -   **模型**：GPT-3.5 和 GPT-4o。
        -   **推理流程**：采用零样本（zero-shot）推理。设计一个标准化的提示（Prompt），告知模型需要执行一个三分类任务，并提供待评估的章节组合文本，要求模型仅返回类别标签数字。为减少结果的随机性，对每个样本进行 5 次预测，取众数作为最终结果。
        -   **输入输出**：输入是包含任务描述和章节组合文本的提示。输出是模型生成的新颖性类别（0, 1, 或 2）。
        -   **优势与局限**：此方法无需训练，可以快速评估大模型的原生能力。局限在于其性能高度依赖于模型的内部知识和提示的设计，且在零样本下可能无法很好地理解特定领域的细粒度评估任务。

-   **重要公式（如有）**
    -   本文未提出新的核心算法公式，主要使用了标准的分类任务评估指标公式。
    -   **加权 F1 分数 (Weighted F1-score)**：用于处理标签不均衡的数据集，根据每个类别的样本数量对其 F1 分数进行加权平均。
        $$\text{Weighted\_F}_1 = \sum_{i} \frac{c_i}{C} \times F_{1i}$$
        其中，$c_i$ 是类别 $i$ 的样本数，$C$ 是总样本数，$F_{1i}$ 是类别 $i$ 的 F1 分数。
    -   **相关性系数**：使用了皮尔逊 (Pearson)、斯皮尔曼 (Spearman) 和肯德尔 (Kendall's tau) 相关性系数，以衡量模型预测分数与真实分数之间的线性及序数关联强度。

### 3. 实验设计与结果（含创新点验证）

-   **实验流程**
    1.  **数据准备**：从 OpenReview 收集 ICLR 2022/2023 论文和评审报告。对评审分数进行一致性处理（保留多位评审员打分差距小于等于1的样本），最终筛选出 6094 篇。与一个已有的解析后数据集匹配，得到 3500 篇可用的论文。
    2.  **标签转换**：将原始的 1-4 分制 TNS 分数合并为 0 (基础新颖), 1 (中等新颖), 2 (高度新颖) 的三分类标签，以应对数据不平衡问题。
    3.  **章节识别**：应用前述的“SciBERT+Llama3+人工”混合方法，为 3500 篇论文的全文内容打上“引言”、“方法”、“结果”、“讨论”的结构化标签。
    4.  **PLM 实验**：将数据集按 8:1:1 划分为训练、验证和测试集。对 16 种章节组合中的每一种，分别在 5 个 PLM 上进行精调和测试。
    5.  **LLM 实验**：为保证公平性，针对每个章节组合，都从三类标签中各随机抽取 40 篇论文（共 120 篇）构成一个平衡的测试子集。使用 GPT-3.5 和 GPT-4o 进行零样本预测。
    6.  **基线对比**：在相同的测试集上，复现并评估了一个传统的、基于引文标题语义距离的新颖性计算方法（Shibayama et al., 2021）。
    7.  **结果分析**：使用准确率（Accuracy）、加权 F1 分数和三种相关性系数，全面评估和对比所有实验结果。

-   **数据集、参数、评价指标**
    -   **数据集**：3500 篇 ICLR 会议论文，其新颖性标签源自专家评审。
    -   **PLM 参数**：训练 10 个周期，初始学习率为 0.0001 并逐步衰减，早停策略的耐心值为 3（即验证集 F1 分数连续 3 个周期未提升则停止训练）。
    -   **评价指标**：准确率 (Accuracy)、加权 F1 分数 (Weighted F1-score)、皮尔逊 (P)、斯皮尔曼 (SP) 和肯德尔 (K) 相关系数。

-   **创新点如何得到验证，结果对比与可视化描述**
    -   **验证一：章节组合优于传统方法**
        -   **结果对比**：实验结果（表6）显示，表现最佳的模型（**SciBERT + IRD 组合**）的准确率达到 **0.6824**，F1 分数为 **0.6515**。而传统的基于引文的基线方法准确率仅为 0.4265，F1 分数为 0.3637。这证明了使用论文核心文本章节进行分析远优于仅依赖引文信息。
    -   **验证二：存在最优章节组合，且优于全文**
        -   **结果对比**：在所有 PLM 实验中（表4），**引言+结果+讨论 (IRD)** 组合始终表现最佳。以 SciBERT 为例，IRD 组合的准确率 (0.6824) 显著高于使用全文 IMRD 的组合 (0.6182)，也高于任何单一章节或其他二元组合。这证实了假设，即并非信息越多越好，一个经过优化的章节组合能够让模型更聚焦于新颖性相关的核心内容。
    -   **验证三：不同模型的能力差异**
        -   **PLM vs. LLM**：精调后的 PLM (最高准确率 0.6824) 显著优于零样本的 LLM (最高准确率 0.4000)。这表明对于新颖性评估这类细粒度的学术任务，当前的 LLM 在没有经过专门微调的情况下是不可靠的。
        -   **LLM 内部问题**：LLM 的结果不仅准确率低，而且表现出强烈的“讨好”偏见，倾向于给出高分（如将低新颖性的论文评为高新颖性）。这在案例分析的可视化图（图7）中得到清晰展示，许多真实标签为 0 的样本被错误预测为 2。
    -   **可视化描述**：通过案例研究（图6和图7）中的混淆热力图，论文直观地展示了 PLM 和 LLM 的预测偏差。颜色深浅代表预测值与真实值的差距。图6显示 PLM 对中低分预测较好，但难以准确预测高分。图7则清晰地揭示了 LLM 将低分误判为高分的系统性偏差。

-   **主要实验结论与作者解释**
    -   **最佳组合**：引言、结果和讨论 (IRD) 的组合是自动化新颖性评估的最佳输入。作者解释，引言部分通常陈述了论文的核心贡献和创新点，而结果和讨论部分则为这些声明提供了具体的证据和深入的阐释，三者结合提供了评估新颖性最全面且相关的上下文。
    -   **重要章节**：引言是所有单一章节中最重要的，其次是结果和讨论。方法（Methods）章节虽然重要，但其内容可能过于技术化和抽象，机器在没有外部知识的情况下难以直接从中有效提取新颖性信号。
    -   **模型局限**：当前的 PLM 和 LLM 在这项任务上仍有很大提升空间，性能远未达到可替代人类专家的水平。PLM 难以识别真正“高度新颖”的论文（可能与训练数据中该类样本少有关），而 LLM 则存在系统性偏见，无法胜任严肃的评估任务。

### 4. 研究结论

-   **重要发现（定量 / 定性）**
    1.  **定量发现**：通过对 16 种章节组合的系统性测试，发现“引言+结果+讨论”（IRD）组合在使用精调的 SciBERT 模型时表现最佳，取得了 0.6824 的准确率和 0.6515 的加权 F1 分数。
    2.  **定性发现**：
        -   对于自动化新颖性评估，并非输入全文（IMRD）就是最好的策略，精选的章节组合可以帮助模型更好地聚焦关键信息。
        -   引言部分是评估新颖性最重要的单一信息来源，结果和讨论部分是其有效的补充。
        -   在零样本设置下，当前的大语言模型（如 GPT-3.5/4o）不足以可靠地执行新颖性评分任务，它们倾向于给出过于乐观的高分。

-   **对学术或应用的意义**
    -   **学术意义**：
        -   为学术文献的自动化评估提供了一种新的、基于文本结构的研究范式。
        -   首次通过大规模实证研究，量化了不同章节对新颖性判断的贡献度。
        -   研究结论对未来结合 PLM 和 LLM 优势来构建更优评估模型的研究方向提供了启示。
    -   **应用意义**：
        -   可以指导自动化同行评审辅助系统的设计，使其优先分析和提取 IRD 章节的内容，以提高效率和准确性。
        -   可为初级研究者或评审人提供一份实用指南，建议他们在评估一篇论文的新颖性时，可以从引言、结果和讨论这三个关键部分入手。

### 5. 创新点列表

1.  **提出并定义了 SC4ANM 任务**：首次将“寻找最优章节组合以进行自动化新颖性预测”明确为一个研究任务，将研究焦点从传统的关键词或引文分析转向了对论文内在结构的深度利用。
2.  **构建了系统的实验框架**：设计并实施了第一个大规模的实证研究，系统地比较了 16 种不同章节组合在精调 PLM 和零样本 LLM 上的表现，提供了全面的基准测试。
3.  **开发了高精度的章节识别方法**：采用了一种结合领域微调模型（SciBERT）、大语言模型（Llama3）和人工校验的混合策略，有效解决了自动化章节识别的准确性问题，为研究的可靠性奠定了基础。
4.  **识别出最优章节组合 (IRD)**：通过实验明确指出“引言+结果+讨论”是预测论文新颖性的最优文本输入，并发现其性能优于使用全文，为该领域提供了具体且可操作的结论。
5.  **揭示了 LLM 在新颖性评估中的局限性**：对前沿的 LLM 在此特定任务上的能力进行了深入评估，发现了它们在零样本场景下的“讨好型”偏见和不稳定性，为学术界和工业界在应用 LLM 进行细粒度评估时提供了重要的警示。